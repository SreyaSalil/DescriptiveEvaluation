{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":22478,"status":"ok","timestamp":1640794106601,"user":{"displayName":"Isha Gupta","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12858761029012429156"},"user_tz":-330},"id":"RfphHxyIIuZB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dfd4b6a6-fa9c-4979-fe66-0f83056f90c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading training data...\n","                                                question1  ... is_duplicate\n","0       Object-Oriented Programming(OOPs) is a type of...  ...            1\n","1       Object-Oriented Programming(OOPs) is a type of...  ...            1\n","2       Object-Oriented Programming(OOPs) is a type of...  ...            1\n","3       Object-Oriented Programming(OOPs) is a type of...  ...            1\n","4       Object-Oriented Programming(OOPs) is a type of...  ...            1\n","...                                                   ...  ...          ...\n","211533  This finalizes () method is called before an o...  ...            0\n","211534  This finalizes () method is called before an o...  ...            0\n","211535  This finalizes () method is called before an o...  ...            0\n","211536  This finalizes () method is called before an o...  ...            0\n","211537  This finalizes () method is called before an o...  ...            0\n","\n","[211538 rows x 3 columns]\n"]}],"source":["    import pandas as pd\n","    lst = []\n","    TRAIN_FILE_NAME = 'Dataset-oops - Sheet1.csv'\n","    print(\"Reading training data...\")\n","    df = pd.read_csv(TRAIN_FILE_NAME)\n","    df.dropna(inplace=True)\n","    start = df[\"EssaySet\"]\n","    # print(type(start))\n","    startText = df[\"EssayText\"]\n","    count=0\n","    for row in df.iterrows():\n","        # print(type(str(row[1][1])), start)\n","        if(str(row[1][1])==str(start) and count!=0):\n","            lst.append([startText, row[1][4], 1])\n","        else:\n","            start = row[1][1]\n","            startText = row[1][4]\n","            count = 0\n","        count+=1\n","    start = df[\"EssaySet\"]\n","    startText = df[\"EssayText\"]\n","    for row in df.iterrows():\n","        for row_in in df.iterrows():\n","            # print(type(str(row[1][1])), start)\n","            if(str(row_in[1][1])!=str(row[1][1])):\n","                lst.append([row[1][4], row_in[1][4], 0])\n","    # lst[800:900]\n","    df_main = pd.DataFrame(lst, columns =['question1', 'question2', 'is_duplicate'], dtype = int) \n","    print(df_main) \n","\n","    df_main = df_main.sample(frac=1, random_state=42).reset_index(drop=True)\n","    df_main\n","    df_main.to_csv('train_main.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n33BeKpdIQHm"},"outputs":[],"source":["# Import required libraries\n","import tensorflow as tf\n","import numpy as np\n","\n","\n","# File paths\n","EMBEDDING_FILE_PATH = 'glove.6B.100d.txt'\n","\n","# Model parameters\n","maxlen = 15             # Maximum number of words in a sentence\n","n_units = 50            # Number of units in LSTM layer\n","clipnorm = 1.5          # Norm for gradient clipping\n","EMBEDDING_DIM = 100     # Dimension of embedding vectors\n","\n","\n","# Function to calculate Manhattan LSTM distance\n","def manh_lstm_distance(question1, question2):\n","  distance = tf.keras.backend.abs(question1-question2)\n","  distance = tf.keras.backend.sum(distance, axis=1, keepdims=True)\n","  distance = -distance\n","  distance = tf.keras.backend.exp(distance)\n","  return distance\n","\n","# Function to create Embedding layer using Tokenizer object\n","def create_embedding_layer(tokenizer):\n","\n","    # Create a dictionary mapping vocabulary words to embedding vectors\n","    embeddings_index = {}\n","    f = open(EMBEDDING_FILE_PATH, encoding=\"utf8\")\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","    f.close()\n","\n","    # Create embedding matrix to initialize Embedding layer\n","    word_index = tokenizer.word_index\n","    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n","    for word, i in word_index.items():\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","\n","    # Create Embedding layer\n","    embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix)\n","    embedding_layer = tf.keras.layers.Embedding(len(word_index)+1,\n","                                                EMBEDDING_DIM,\n","                                                embeddings_initializer=embeddings_initializer,\n","                                                input_length=maxlen,\n","                                                trainable=True,\n","                                                name=\"embedding_layer\")\n","\n","    return embedding_layer\n","\n","# Function to create Siamese Manhattan LSTM model\n","def create(tokenizer):\n","    print(\"Creating model...\")\n","    embedding_layer = create_embedding_layer(tokenizer)\n","    question1 = tf.keras.layers.Input(shape=(maxlen,), dtype='int32', name=\"question1\")\n","    question2 = tf.keras.layers.Input(shape=(maxlen,), dtype='int32', name=\"question2\")\n","    question1_encoded = embedding_layer(question1)\n","    question2_encoded = embedding_layer(question2)\n","    common_lstm_layer = tf.keras.layers.LSTM(n_units, name=\"common_lstm_layer\")\n","    question1_output = common_lstm_layer(question1_encoded)\n","    question2_output = common_lstm_layer(question2_encoded)\n","    manhattan_lstm_distance = tf.keras.layers.Lambda(lambda x: manh_lstm_distance(x[0], x[1]), name=\"manhattan_lstm_distance\")([question1_output, question2_output])\n","    model = tf.keras.models.Model([question1, question2], manhattan_lstm_distance)\n","    loss = 'binary_crossentropy'\n","    optimizer = tf.keras.optimizers.Adam(clipnorm=clipnorm)\n","    metrics = ['accuracy', 'mse']\n","    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":678,"status":"ok","timestamp":1640695806282,"user":{"displayName":"Isha Gupta","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12858761029012429156"},"user_tz":-330},"id":"56wvTD8yIUYP","outputId":"307132a4-17c6-482a-f8fc-f22d2c51e879"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["# Import required libraries\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer \n","\n","# Prerequisites for cleaning\n","nltk.download(\"stopwords\")                      # Download stopwords from NLTK library\n","nltk.download('wordnet')                        # Download wordnet, a lexixal database from NLTK library\n","stopwords = set(stopwords.words('english'))     # Store stopwords\n","lemmatizer = WordNetLemmatizer()                # Create object for lemmatization\n","\n","# Function for standard cleaning of text (remove punctuations, abbreviations, etc.) using regular expressions\n","def standard_clean(text):\n","  text = str(text)\n","  text = text.lower()\n","  text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n","  text = re.sub(r\"what's\", \"what is \", text)\n","  text = re.sub(r\"\\'s\", \" \", text)\n","  text = re.sub(r\"\\'ve\", \" have \", text)\n","  text = re.sub(r\"can't\", \"cannot \", text)\n","  text = re.sub(r\"n't\", \" not \", text)\n","  text = re.sub(r\"i'm\", \"i am \", text)\n","  text = re.sub(r\"\\'re\", \" are \", text)\n","  text = re.sub(r\"\\'d\", \" would \", text)\n","  text = re.sub(r\"\\'ll\", \" will \", text)\n","  text = re.sub(r\",\", \" \", text)\n","  text = re.sub(r\"\\.\", \" \", text)\n","  text = re.sub(r\"!\", \" ! \", text)\n","  text = re.sub(r\"\\/\", \" \", text)\n","  text = re.sub(r\"\\^\", \" ^ \", text)\n","  text = re.sub(r\"\\+\", \" + \", text)\n","  text = re.sub(r\"\\-\", \" - \", text)\n","  text = re.sub(r\"\\=\", \" = \", text)\n","  text = re.sub(r\"'\", \" \", text)\n","  text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n","  text = re.sub(r\":\", \" : \", text)\n","  text = re.sub(r\" e g \", \" eg \", text)\n","  text = re.sub(r\" b g \", \" bg \", text)\n","  text = re.sub(r\" u s \", \" american \", text)\n","  text = re.sub(r\"\\0s\", \"0\", text)\n","  text = re.sub(r\" 9 11 \", \"911\", text)\n","  text = re.sub(r\"e - mail\", \"email\", text)\n","  text = re.sub(r\"j k\", \"jk\", text)\n","  text = re.sub(r\"\\s{2,}\", \" \", text)\n","  return text\n","\n","# Function to remove stopwords from a sentence\n","def remove_stopwords(text):\n","  text = text.split()\n","  clean = \"\"\n","  for w in text:\n","    if w not in stopwords:\n","      clean = clean + \" \" + w\n","  return str(clean[1:])\n","\n","# Function to lemmatize words of a sentence using Lemmatizer object\n","def lemmatize(text):\n","  text = text.split()\n","  clean = \"\"\n","  for w in text:\n","    clean = clean + \" \" + lemmatizer.lemmatize(w)\n","  return str(clean[1:])\n","\n","# Function to clean the text\n","def clean(text):\n","  text = standard_clean(text)\n","  text = remove_stopwords(text)\n","  text = lemmatize(text)\n","  return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z1VdliUEIdQv"},"outputs":[],"source":["# Import required libraries\n","import tensorflow as tf\n","import numpy as np\n","import pickle           # Library to load Tokenizer object\n","\n","# Import local modules\n","# import clean_text as ct\n","\n","# Preprocessing parameters\n","maxlen=15        # Maximum number of words in a processed question\n","\n","\n","# Function to create Tokenizer object\n","def tokenize(df):\n","    df['concatenated'] = df['question1'] + \" \" + df['question2']\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n","    tokenizer.fit_on_texts(df.concatenated)\n","    return tokenizer\n","\n","# Function to preprocess textual data\n","def preprocess(df, mode):\n","\n","    # Clean the text\n","    df['question1'] = df.question1.map(lambda x: clean(x))\n","    df['question2'] = df.question2.map(lambda x: clean(x))\n","\n","    # Prepare the data for the model\n","    print(\"Preparing data for model...\")\n","\n","    # While training, create Tokenizer object and also return labels\n","    if mode=='train':\n","        tokenizer = tokenize(df)\n","        df['question1'] = tokenizer.texts_to_sequences(df.question1)\n","        df['question2'] = tokenizer.texts_to_sequences(df.question2)\n","        question1 = np.array(list(tf.keras.preprocessing.sequence.pad_sequences(df.question1, maxlen=maxlen)))\n","        question2 = np.array(list(tf.keras.preprocessing.sequence.pad_sequences(df.question2, maxlen=maxlen)))\n","        labels = np.array(list(df.is_duplicate))\n","        return question1, question2, labels, tokenizer\n","\n","    # While predicting, load existing Tokenizer object\n","    if mode=='predict':\n","        with open('../checkpoints/tokenizer.pickle', 'rb') as handle:\n","            tokenizer = pickle.load(handle)\n","        df['question1'] = tokenizer.texts_to_sequences(df.question1)\n","        df['question2'] = tokenizer.texts_to_sequences(df.question2)\n","        question1 = np.array(list(tf.keras.preprocessing.sequence.pad_sequences(df.question1, maxlen=maxlen)))\n","        question2 = np.array(list(tf.keras.preprocessing.sequence.pad_sequences(df.question2, maxlen=maxlen)))\n","        return question1, question2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_dtBxS04Ifyh"},"outputs":[],"source":["# Import required libraries\n","import pandas as pd\n","\n","# Import local modules\n","# import preprocess_data as pp\n","\n","# File paths\n","# DATASET_PATH = '../dataset'\n","TRAIN_FILE_NAME = 'train_main.csv'\n","\n","# Function to load, process training data and create Tokenizer onject\n","def load():\n","    print(\"Reading training data...\")\n","    df = pd.read_csv(TRAIN_FILE_NAME)\n","\n","    print(\"Preprocessing training data...\")\n","    question1, question2, labels, tokenizer = preprocess(df, mode='train')\n","\n","    return question1, question2, labels, tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kbPyBMlcIhzi","outputId":"1cfd5ae3-15f0-4482-e33f-dc0903a72798","executionInfo":{"status":"ok","timestamp":1640696076404,"user_tz":-330,"elapsed":264334,"user":{"displayName":"Isha Gupta","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12858761029012429156"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading training data...\n","Preprocessing training data...\n","Preparing data for model...\n","Creating model...\n","Training model...\n","Epoch 1/2\n","2645/2645 [==============================] - 79s 28ms/step - loss: 0.0128 - accuracy: 0.9980 - mse: 0.0020 - val_loss: 0.0129 - val_accuracy: 0.9978 - val_mse: 0.0021\n","Epoch 2/2\n","2645/2645 [==============================] - 74s 28ms/step - loss: 0.0094 - accuracy: 0.9980 - mse: 0.0019 - val_loss: 0.0128 - val_accuracy: 0.9978 - val_mse: 0.0021\n","Saving model and model weights...\n","Saving Tokenizer object...\n"]}],"source":["# Library to save Tokenizer object\n","import pickle\n","\n","# Import local modules\n","# import load_data as ld\n","# import create_model as cm\n","\n","# File paths\n","CHECKPOINT_PATH = '../checkpoints'\n","WEIGHTS_FILE_NAME = '/weights'\n","MODEL_FILE_NAME = '/model.h5'\n","TOKENIZER_FILE_NAME = '/tokenizer.pickle'\n","\n","# Training parameters\n","epochs = 2                  # Number of epochs\n","batch_size = 64             # Training batch size\n","validation_split=0.2        # Fraction of training data for validation\n","verbose=1                   # Show progress bar\n","\n","# Load, process training data and create Tokenizer onject\n","question1, question2, labels, tokenizer = load()\n","\n","# Create model using Tokenizer object\n","model = create(tokenizer)\n","\n","# Train the model\n","print(\"Training model...\")\n","model.fit([question1, question2], labels, epochs=epochs, batch_size=batch_size, validation_split=validation_split, verbose=verbose)\n","\n","# Save model and model weights\n","print(\"Saving model and model weights...\")\n","model.save(CHECKPOINT_PATH + MODEL_FILE_NAME)\n","model.save_weights(CHECKPOINT_PATH + WEIGHTS_FILE_NAME)\n","\n","# Save Tokenizer object\n","print(\"Saving Tokenizer object...\")\n","with open(CHECKPOINT_PATH + TOKENIZER_FILE_NAME, 'wb') as handle:\n","    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L8qdX9s_Ij29","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640696395276,"user_tz":-330,"elapsed":3280,"user":{"displayName":"Isha Gupta","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12858761029012429156"}},"outputId":"cf72874f-b895-4eab-9e05-520b98334d27"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model...\n","Reading test data...\n","0       Dynamic (run time) polymorphism is the polymor...\n","1       A virtual function or virtual method in an OOP...\n","2       new: Allocates memory for the object on the fr...\n","3       Overloading occurs when two or more methods in...\n","4       An interface is most certainly not a blueprint...\n","                              ...                        \n","1995    A virtual function is a member function that y...\n","1996    Coupling as explained by Craig Cruden is in my...\n","1997    Code that is organized by functionality and do...\n","1998    Encapsulation is a method to hide the data in ...\n","1999    Object-oriented programming mainly focuses on ...\n","Name: question1, Length: 2000, dtype: object\n","0       Constructor is piece of code which we are usin...\n","1       Within a single program, output of a function ...\n","2       Object is an instance of a class. An object in...\n","3       OOPs allows us to hide implementation details ...\n","4       An interface is better than a abstract class w...\n","                              ...                        \n","1995    A blueprint typically includes the internals. ...\n","1996    An exception is an error. The long answer: An ...\n","1997    Declaring a method in sub class which is alrea...\n","1998    cohesion refers all about how a single class i...\n","1999    Garbage Collection is process of reclaiming th...\n","Name: question2, Length: 2000, dtype: object\n","Preprocessing test data...\n","Preparing data for model...\n","Predicting Manhattan LSTM distances...\n","63/63 [==============================] - 1s 5ms/step\n","Making binary predictions...\n","     Manhattan LSTM distances                    Score Prediction\n","0             [1.7697905e-05]   [0.017697904695523903]        [0]\n","1              [5.726438e-05]    [0.05726438030251302]        [0]\n","2             [0.00017244455]    [0.17244454647880048]        [0]\n","3             [2.8042186e-06]  [0.0028042186386301182]        [0]\n","4               [0.001564175]     [1.5641750069335103]        [1]\n","...                       ...                      ...        ...\n","1995          [4.8552443e-05]    [0.04855244333157316]        [0]\n","1996          [1.0148292e-05]   [0.010148291949008126]        [0]\n","1997          [5.0996514e-06]   [0.005099651389173232]        [0]\n","1998          [1.9850204e-05]     [0.0198502038983861]        [0]\n","1999            [3.93644e-05]     [0.0393644004361704]        [0]\n","\n","[2000 rows x 3 columns]\n"]}],"source":["# Import required libraries\n","import tensorflow as tf\n","import pandas as pd\n","\n","# from create_model import manh_lstm_distance\n","\n","# File paths\n","CHECKPOINT_PATH = '../checkpoints'\n","MODEL_FILE_NAME = '/model.h5'\n","DATASET_PATH = '../dataset'\n","TEST_FILE_NAME = \"train_main.csv\"\n","\n","# Prediction parameters\n","skiprows = 0            # Predict labels for question pairs from index 'skiprows'\n","nrows = 2000           # to index 'skiprows + nrows' in the test file\n","\n","threshold = 0.001         # Minimum Manhattan LSTM distance between two outputs\n","                        # for them to be classified as semantically similar\n","\n","\n","# Load trained model\n","print(\"Loading model...\")\n","model = tf.keras.models.load_model(CHECKPOINT_PATH + MODEL_FILE_NAME, custom_objects={\"manh_lstm_distance\": manh_lstm_distance})\n","\n","# Read test file\n","print(\"Reading test data...\")\n","df = pd.read_csv(TEST_FILE_NAME, skiprows=skiprows, nrows=nrows)\n","l1 = df['question1'].tolist()\n","l2 = df['question2'].tolist()\n","print(df['question1'])\n","print(df['question2'])\n","# Preprocess test data\n","print(\"Preprocessing test data...\")\n","question1, question2 = preprocess(df, mode='predict')\n","\n","# Predict Manhattan LSTM distances\n","print(\"Predicting Manhattan LSTM distances...\")\n","manh_lstm_distance = model.predict([question1, question2], verbose=1)\n","\n","# Make binary predictions\n","print(\"Making binary predictions...\")\n","prediction = manh_lstm_distance>threshold\n","prediction = prediction.astype(int)\n","\n","# Print predictions\n","score = (manh_lstm_distance*1000000)/1000\n","data = {'Manhattan LSTM distances': list(manh_lstm_distance), 'Score': list(score), 'Prediction': list(prediction)}\n","df1 = pd.DataFrame(data)\n","print(df1)"]},{"cell_type":"code","source":[""],"metadata":{"id":"6evUYbac8P0y"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"lstm.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}