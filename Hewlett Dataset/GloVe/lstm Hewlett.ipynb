{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rh_AXXWX6_XJ","executionInfo":{"status":"ok","timestamp":1651679757960,"user_tz":-330,"elapsed":3842,"user":{"displayName":"Sreya Salil","userId":"05468731947198838952"}},"outputId":"386eb6dc-e71d-4843-b8e2-e82172869646"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Engineering/Curriculum/8th Semester/Internship/descriptive_evaluation_project/Hewlett Dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pMzcjEsM7R2P","executionInfo":{"status":"ok","timestamp":1651679757962,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sreya Salil","userId":"05468731947198838952"}},"outputId":"c44a184d-60fd-4ad9-f588-3100335c5b50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/17Gn89Edqfyxljr8tO09VdcQWGcUCa_Ua/descriptive_evaluation_project/Hewlett Dataset\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RfphHxyIIuZB","colab":{"base_uri":"https://localhost:8080/","height":424},"outputId":"37a528f7-003d-4110-88bd-31dee3af8b6b","executionInfo":{"status":"ok","timestamp":1651679760242,"user_tz":-330,"elapsed":2287,"user":{"displayName":"Sreya Salil","userId":"05468731947198838952"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                question1  \\\n","0       Dynamic (run time) polymorphism is the polymor...   \n","1       A virtual function or virtual method in an OOP...   \n","2       new: Allocates memory for the object on the fr...   \n","3       Overloading occurs when two or more methods in...   \n","4       An interface is most certainly not a blueprint...   \n","...                                                   ...   \n","211533  OOPs allows us to hide implementation details ...   \n","211534  When Java encounters an exception, it throws i...   \n","211535  Copy constructor is called when a new object i...   \n","211536  Dynamic Polymorphism(Late Binding/ Runtime Pol...   \n","211537  OOps, concepts in java is to improve code read...   \n","\n","                                                question2  is_duplicate  \n","0       Constructor is piece of code which we are usin...             0  \n","1       Within a single program, output of a function ...             0  \n","2       Object is an instance of a class. An object in...             0  \n","3       OOPs allows us to hide implementation details ...             0  \n","4       An interface is better than a abstract class w...             0  \n","...                                                   ...           ...  \n","211533  Exception is an abnormal condition. In Java, a...             0  \n","211534  An interface is a completely \"abstract class\" ...             0  \n","211535  Overriding occurs when two methods have the sa...             0  \n","211536  The interface consists of the signatures of me...             0  \n","211537  Polymorphism is an object-oriented programming...             0  \n","\n","[211538 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-35d23a8e-1480-4c89-89dd-06be84a86b32\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question1</th>\n","      <th>question2</th>\n","      <th>is_duplicate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Dynamic (run time) polymorphism is the polymor...</td>\n","      <td>Constructor is piece of code which we are usin...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A virtual function or virtual method in an OOP...</td>\n","      <td>Within a single program, output of a function ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>new: Allocates memory for the object on the fr...</td>\n","      <td>Object is an instance of a class. An object in...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Overloading occurs when two or more methods in...</td>\n","      <td>OOPs allows us to hide implementation details ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>An interface is most certainly not a blueprint...</td>\n","      <td>An interface is better than a abstract class w...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>211533</th>\n","      <td>OOPs allows us to hide implementation details ...</td>\n","      <td>Exception is an abnormal condition. In Java, a...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>211534</th>\n","      <td>When Java encounters an exception, it throws i...</td>\n","      <td>An interface is a completely \"abstract class\" ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>211535</th>\n","      <td>Copy constructor is called when a new object i...</td>\n","      <td>Overriding occurs when two methods have the sa...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>211536</th>\n","      <td>Dynamic Polymorphism(Late Binding/ Runtime Pol...</td>\n","      <td>The interface consists of the signatures of me...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>211537</th>\n","      <td>OOps, concepts in java is to improve code read...</td>\n","      <td>Polymorphism is an object-oriented programming...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>211538 rows Ã— 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35d23a8e-1480-4c89-89dd-06be84a86b32')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-35d23a8e-1480-4c89-89dd-06be84a86b32 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-35d23a8e-1480-4c89-89dd-06be84a86b32');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":47}],"source":["       \n","    # lst = []\n","    # TRAIN_FILE_NAME = 'Hewlett Dataset.csv'\n","    # print(\"Reading training data...\")\n","    # df = pd.read_csv(TRAIN_FILE_NAME)\n","    # df.dropna(inplace=True)\n","    # start = df[\"EssaySet\"]\n","    # # print(type(start))\n","    # startText = df[\"EssayText\"]\n","    # count=0\n","    # for row in df.iterrows():\n","    #     # print(type(str(row[1][1])), start)\n","    #     if(str(row[1][1])==str(start) and count!=0):\n","    #         lst.append([startText, row[1][4], 1])\n","    #     else:\n","    #         start = row[1][1]\n","    #         startText = row[1][4]\n","    #         count = 0\n","    #     count+=1\n","    # start = df[\"EssaySet\"]\n","    # startText = df[\"EssayText\"]\n","    # for row in df.iterrows():\n","    #     for row_in in df.iterrows():\n","    #         # print(type(str(row[1][1])), start)\n","    #         if(str(row_in[1][1])!=str(row[1][1])):\n","    #             lst.append([row[1][4], row_in[1][4], 0])\n","    # # lst[800:900]\n","    # df_main = pd.DataFrame(lst, columns =['question1', 'question2', 'is_duplicate'], dtype = int) \n","    # print(df_main) \n","\n","    # df_main = df_main.sample(frac=1, random_state=42).reset_index(drop=True)\n","    # df_main\n","    # df_main.to_csv('train_main.csv')\n","import pandas as pd\n","TRAIN_FILE_NAME = 'train_oop.csv'\n","df_main = pd.read_csv(TRAIN_FILE_NAME, encoding='latin1')\n","df_main"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n33BeKpdIQHm"},"outputs":[],"source":["# Import required libraries\n","import tensorflow as tf\n","import numpy as np\n","\n","\n","# File paths\n","EMBEDDING_FILE_PATH = 'glove.6B.100d.txt'\n","\n","# Model parameters\n","maxlen = 15             # Maximum number of words in a sentence\n","n_units = 50            # Number of units in LSTM layer\n","clipnorm = 1.5          # Norm for gradient clipping\n","EMBEDDING_DIM = 100     # Dimension of embedding vectors\n","\n","\n","# Function to calculate Manhattan LSTM distance\n","def manh_lstm_distance(question1, question2):\n","  distance = tf.keras.backend.abs(question1-question2)\n","  distance = tf.keras.backend.sum(distance, axis=1, keepdims=True)\n","  distance = -distance\n","  distance = tf.keras.backend.exp(distance)\n","  return distance\n","\n","# Function to create Embedding layer using Tokenizer object\n","def create_embedding_layer(tokenizer):\n","\n","    # Create a dictionary mapping vocabulary words to embedding vectors\n","    embeddings_index = {}\n","    f = open(EMBEDDING_FILE_PATH, encoding=\"utf8\")\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","    f.close()\n","\n","    # Create embedding matrix to initialize Embedding layer\n","    word_index = tokenizer.word_index\n","    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n","    for word, i in word_index.items():\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","\n","    # Create Embedding layer\n","    embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix)\n","    embedding_layer = tf.keras.layers.Embedding(len(word_index)+1,\n","                                                EMBEDDING_DIM,\n","                                                embeddings_initializer=embeddings_initializer,\n","                                                input_length=maxlen,\n","                                                trainable=True,\n","                                                name=\"embedding_layer\")\n","\n","    return embedding_layer\n","\n","# Function to create Siamese Manhattan LSTM model\n","def create(tokenizer):\n","    print(\"Creating model...\")\n","    embedding_layer = create_embedding_layer(tokenizer)\n","    question1 = tf.keras.layers.Input(shape=(maxlen,), dtype='int32', name=\"question1\")\n","    question2 = tf.keras.layers.Input(shape=(maxlen,), dtype='int32', name=\"question2\")\n","    question1_encoded = embedding_layer(question1)\n","    question2_encoded = embedding_layer(question2)\n","    common_lstm_layer = tf.keras.layers.LSTM(n_units, name=\"common_lstm_layer\")\n","    question1_output = common_lstm_layer(question1_encoded)\n","    question2_output = common_lstm_layer(question2_encoded)\n","    manhattan_lstm_distance = tf.keras.layers.Lambda(lambda x: manh_lstm_distance(x[0], x[1]), name=\"manhattan_lstm_distance\")([question1_output, question2_output])\n","    model = tf.keras.models.Model([question1, question2], manhattan_lstm_distance)\n","    loss = 'binary_crossentropy'\n","    optimizer = tf.keras.optimizers.Adam(clipnorm=clipnorm)\n","    metrics = ['accuracy', 'mse']\n","    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"56wvTD8yIUYP","outputId":"58429098-ebb5-47de-e607-e6e69395cb9b","executionInfo":{"status":"ok","timestamp":1651679760245,"user_tz":-330,"elapsed":24,"user":{"displayName":"Sreya Salil","userId":"05468731947198838952"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["# Import required libraries\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer \n","\n","# Prerequisites for cleaning\n","nltk.download(\"stopwords\")                      # Download stopwords from NLTK library\n","nltk.download('wordnet')                        # Download wordnet, a lexixal database from NLTK library\n","stopwords = set(stopwords.words('english'))     # Store stopwords\n","lemmatizer = WordNetLemmatizer()                # Create object for lemmatization\n","\n","# Function for standard cleaning of text (remove punctuations, abbreviations, etc.) using regular expressions\n","def standard_clean(text):\n","  text = str(text)\n","  text = text.lower()\n","  text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n","  text = re.sub(r\"what's\", \"what is \", text)\n","  text = re.sub(r\"\\'s\", \" \", text)\n","  text = re.sub(r\"\\'ve\", \" have \", text)\n","  text = re.sub(r\"can't\", \"cannot \", text)\n","  text = re.sub(r\"n't\", \" not \", text)\n","  text = re.sub(r\"i'm\", \"i am \", text)\n","  text = re.sub(r\"\\'re\", \" are \", text)\n","  text = re.sub(r\"\\'d\", \" would \", text)\n","  text = re.sub(r\"\\'ll\", \" will \", text)\n","  text = re.sub(r\",\", \" \", text)\n","  text = re.sub(r\"\\.\", \" \", text)\n","  text = re.sub(r\"!\", \" ! \", text)\n","  text = re.sub(r\"\\/\", \" \", text)\n","  text = re.sub(r\"\\^\", \" ^ \", text)\n","  text = re.sub(r\"\\+\", \" + \", text)\n","  text = re.sub(r\"\\-\", \" - \", text)\n","  text = re.sub(r\"\\=\", \" = \", text)\n","  text = re.sub(r\"'\", \" \", text)\n","  text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n","  text = re.sub(r\":\", \" : \", text)\n","  text = re.sub(r\" e g \", \" eg \", text)\n","  text = re.sub(r\" b g \", \" bg \", text)\n","  text = re.sub(r\" u s \", \" american \", text)\n","  text = re.sub(r\"\\0s\", \"0\", text)\n","  text = re.sub(r\" 9 11 \", \"911\", text)\n","  text = re.sub(r\"e - mail\", \"email\", text)\n","  text = re.sub(r\"j k\", \"jk\", text)\n","  text = re.sub(r\"\\s{2,}\", \" \", text)\n","  return text\n","\n","# Function to remove stopwords from a sentence\n","def remove_stopwords(text):\n","  text = text.split()\n","  clean = \"\"\n","  for w in text:\n","    if w not in stopwords:\n","      clean = clean + \" \" + w\n","  return str(clean[1:])\n","\n","# Function to lemmatize words of a sentence using Lemmatizer object\n","def lemmatize(text):\n","  text = text.split()\n","  clean = \"\"\n","  for w in text:\n","    clean = clean + \" \" + lemmatizer.lemmatize(w)\n","  return str(clean[1:])\n","\n","# Function to clean the text\n","def clean(text):\n","  text = standard_clean(text)\n","  text = remove_stopwords(text)\n","  text = lemmatize(text)\n","  return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z1VdliUEIdQv"},"outputs":[],"source":["# Import required libraries\n","import tensorflow as tf\n","import numpy as np\n","import pickle           # Library to load Tokenizer object\n","\n","# Import local modules\n","# import clean_text as ct\n","\n","# Preprocessing parameters\n","maxlen=15        # Maximum number of words in a processed question\n","\n","\n","# Function to create Tokenizer object\n","def tokenize(df):\n","    df['concatenated'] = df['question1'] + \" \" + df['question2']\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n","    tokenizer.fit_on_texts(df.concatenated)\n","    return tokenizer\n","\n","# Function to preprocess textual data\n","def preprocess(df, mode):\n","\n","    # Clean the text\n","    df['question1'] = df.question1.map(lambda x: clean(x))\n","    df['question2'] = df.question2.map(lambda x: clean(x))\n","\n","    # Prepare the data for the model\n","    print(\"Preparing data for model...\")\n","\n","    # While training, create Tokenizer object and also return labels\n","    if mode=='train':\n","        tokenizer = tokenize(df)\n","        df['question1'] = tokenizer.texts_to_sequences(df.question1)\n","        df['question2'] = tokenizer.texts_to_sequences(df.question2)\n","        question1 = np.array(list(tf.keras.preprocessing.sequence.pad_sequences(df.question1, maxlen=maxlen)))\n","        question2 = np.array(list(tf.keras.preprocessing.sequence.pad_sequences(df.question2, maxlen=maxlen)))\n","        labels = np.array(list(df.is_duplicate))\n","        return question1, question2, labels, tokenizer\n","\n","    # While predicting, load existing Tokenizer object\n","    if mode=='predict':\n","        with open('../checkpoints/tokenizer.pickle', 'rb') as handle:\n","            tokenizer = pickle.load(handle)\n","        df['question1'] = tokenizer.texts_to_sequences(df.question1)\n","        df['question2'] = tokenizer.texts_to_sequences(df.question2)\n","        question1 = np.array(list(tf.keras.preprocessing.sequence.pad_sequences(df.question1, maxlen=maxlen)))\n","        question2 = np.array(list(tf.keras.preprocessing.sequence.pad_sequences(df.question2, maxlen=maxlen)))\n","        return question1, question2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_dtBxS04Ifyh"},"outputs":[],"source":["# Import required libraries\n","import pandas as pd\n","\n","# Import local modules\n","# import preprocess_data as pp\n","\n","# File paths\n","# DATASET_PATH = '../dataset'\n","TRAIN_FILE_NAME = 'train_oop.csv'\n","\n","# Function to load, process training data and create Tokenizer onject\n","def load():\n","    print(\"Reading training data...\")\n","    df = pd.read_csv(TRAIN_FILE_NAME, encoding='latin1')\n","\n","    print(\"Preprocessing training data...\")\n","    question1, question2, labels, tokenizer = preprocess(df, mode='train')\n","\n","    return question1, question2, labels, tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kbPyBMlcIhzi","outputId":"4eda32ff-9f0c-4017-bfaa-e3c3af869040","executionInfo":{"status":"ok","timestamp":1651680155273,"user_tz":-330,"elapsed":330621,"user":{"displayName":"Sreya Salil","userId":"05468731947198838952"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading training data...\n","Preprocessing training data...\n","Preparing data for model...\n","Creating model...\n","Training model...\n","Epoch 1/2\n","2645/2645 [==============================] - 95s 34ms/step - loss: 0.0127 - accuracy: 0.9980 - mse: 0.0020 - val_loss: 0.0133 - val_accuracy: 0.9978 - val_mse: 0.0021\n","Epoch 2/2\n","2645/2645 [==============================] - 86s 32ms/step - loss: 0.0096 - accuracy: 0.9980 - mse: 0.0019 - val_loss: 0.0125 - val_accuracy: 0.9978 - val_mse: 0.0020\n","Saving model and model weights...\n","Saving Tokenizer object...\n"]}],"source":["# Library to save Tokenizer object\n","import pickle\n","\n","# Import local modules\n","# import load_data as ld\n","# import create_model as cm\n","\n","# File paths\n","CHECKPOINT_PATH = '../checkpoints'\n","WEIGHTS_FILE_NAME = '/weights'\n","MODEL_FILE_NAME = '/model.h5'\n","TOKENIZER_FILE_NAME = '/tokenizer.pickle'\n","\n","# Training parameters\n","epochs = 2                  # Number of epochs\n","batch_size = 64             # Training batch size\n","validation_split=0.2        # Fraction of training data for validation\n","verbose=1                   # Show progress bar\n","\n","# Load, process training data and create Tokenizer onject\n","question1, question2, labels, tokenizer = load()\n","\n","# Create model using Tokenizer object\n","model = create(tokenizer)\n","\n","# Train the model\n","print(\"Training model...\")\n","model.fit([question1, question2], labels, epochs=epochs, batch_size=batch_size, validation_split=validation_split, verbose=verbose)\n","\n","# Save model and model weights\n","print(\"Saving model and model weights...\")\n","model.save(CHECKPOINT_PATH + MODEL_FILE_NAME)\n","model.save_weights(CHECKPOINT_PATH + WEIGHTS_FILE_NAME)\n","\n","# Save Tokenizer object\n","print(\"Saving Tokenizer object...\")\n","with open(CHECKPOINT_PATH + TOKENIZER_FILE_NAME, 'wb') as handle:\n","    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L8qdX9s_Ij29","colab":{"base_uri":"https://localhost:8080/"},"outputId":"96d4dab3-28f1-4a33-a17c-28ec19f2bfb1","executionInfo":{"status":"ok","timestamp":1651680162743,"user_tz":-330,"elapsed":7113,"user":{"displayName":"Sreya Salil","userId":"05468731947198838952"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model...\n","Reading test data...\n","0       First you would need to know what the samples ...\n","1       You would need the starting mass, ending mass,...\n","2       To replicate these students experiment you wou...\n","3       To replicate the experiment you would need to ...\n","4       The additonal information youwould need to kno...\n","                              ...                        \n","1995    To replicate this experiment I would need to k...\n","1996    The additional information I would need to rep...\n","1997    The procedure could be better  if they include...\n","1998    After reading this procedure i think it would ...\n","1999    Pour vinegar in each of four separate, but ide...\n","Name: question1, Length: 2000, dtype: object\n","0       To replicate the experiments you will need all...\n","1       1. Have a determined size for the starting sam...\n","2       After reading the groups procedure the additio...\n","3       You will need the type of vinegar and know the...\n","4       You need to add to number 4, it should say how...\n","                              ...                        \n","1995    To replicate their experiment I would need to ...\n","1996    You would need to add observations, you would ...\n","1997    To me I think that this experiment will need t...\n","1998    To replicate the experiment, they would need t...\n","1999    For this experiment to be repllcated there wou...\n","Name: question2, Length: 2000, dtype: object\n","Preprocessing test data...\n","Preparing data for model...\n","Predicting Manhattan LSTM distances...\n","63/63 [==============================] - 1s 5ms/step\n","Making binary predictions...\n","     Manhattan LSTM distances        Score Prediction\n","0               [0.009357011]   [9.357011]        [1]\n","1               [0.041446406]  [41.446407]        [1]\n","2               [0.004403442]  [4.4034424]        [1]\n","3                [0.04234246]   [42.34246]        [1]\n","4               [0.050723746]  [50.723747]        [1]\n","...                       ...          ...        ...\n","1995            [0.011617944]  [11.617944]        [1]\n","1996            [0.039687827]  [39.687828]        [1]\n","1997             [0.03204548]   [32.04548]        [1]\n","1998           [0.0065884413]  [6.5884414]        [1]\n","1999            [0.041399397]    [41.3994]        [1]\n","\n","[2000 rows x 3 columns]\n"]}],"source":["# Import required libraries\n","import tensorflow as tf\n","import pandas as pd\n","\n","# from create_model import manh_lstm_distance\n","\n","# File paths\n","CHECKPOINT_PATH = '../checkpoints'\n","MODEL_FILE_NAME = '/model.h5'\n","DATASET_PATH = '../dataset'\n","TEST_FILE_NAME = \"train_main.csv\"\n","\n","# Prediction parameters\n","skiprows = 0            # Predict labels for question pairs from index 'skiprows'\n","nrows = 2000           # to index 'skiprows + nrows' in the test file\n","\n","threshold = 0.001         # Minimum Manhattan LSTM distance between two outputs\n","                        # for them to be classified as semantically similar\n","\n","\n","# Load trained model\n","print(\"Loading model...\")\n","model = tf.keras.models.load_model(CHECKPOINT_PATH + MODEL_FILE_NAME, custom_objects={\"manh_lstm_distance\": manh_lstm_distance})\n","\n","# Read test file\n","print(\"Reading test data...\")\n","df = pd.read_csv(TEST_FILE_NAME, skiprows=skiprows, nrows=nrows)\n","l1 = df['question1'].tolist()\n","l2 = df['question2'].tolist()\n","print(df['question1'])\n","print(df['question2'])\n","# Preprocess test data\n","print(\"Preprocessing test data...\")\n","question1, question2 = preprocess(df, mode='predict')\n","\n","# Predict Manhattan LSTM distances\n","print(\"Predicting Manhattan LSTM distances...\")\n","manh_lstm_distance = model.predict([question1, question2], verbose=1)\n","\n","# Make binary predictions\n","print(\"Making binary predictions...\")\n","prediction = manh_lstm_distance>threshold\n","prediction = prediction.astype(int)\n","\n","# Print predictions\n","score = manh_lstm_distance*1000\n","data = {'Manhattan LSTM distances': list(manh_lstm_distance), 'Score': list(score), 'Prediction': list(prediction)}\n","df1 = pd.DataFrame(data)\n","print(df1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lPgFir1XJQMV"},"outputs":[],"source":[""]}],"metadata":{"colab":{"name":"lstm Hewlett.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}