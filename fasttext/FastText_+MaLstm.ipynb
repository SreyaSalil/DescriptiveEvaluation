{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FastText +MaLstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# File paths\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UC7PBf8oRpGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34162002-6ced-44aa-d7f9-3ea731a57553"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "c0PpMr5bNOHs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4b00901-7a4d-4421-9d3f-8ee88377bbbc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from gensim.models.wrappers import FastText\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "import datetime\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, LSTM, Lambda\n",
        "import keras.backend as K\n",
        "# from keras.optimizers import Adadelta\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "Xh0RcDzYK4CP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/facebookresearch/fastText.git\n",
        "!cd fastText\n",
        "!pip install ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnDvxCgWKBx5",
        "outputId": "e5de92a6-8fd7-4dde-b199-a06c0612a48d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 0: cd: fastText: No such file or directory\n",
            "\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYtioXLwMZop",
        "outputId": "74508793-3fc4-4207-9c99-8e9a04e0d00d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3143427 sha256=5ab863ddaddcf2a6ae27776d4ae1ad7f088232c1f0b07cfcf559cd9c5856707c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext"
      ],
      "metadata": {
        "id": "0qYvpwyKLB-l"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_CSV = '/content/drive/MyDrive/DesEval/train_main.csv'\n",
        "TEST_CSV = '/content/drive/MyDrive/DesEval/train_main.csv'\n",
        "EMBEDDING_FILE = '/content/drive/MyDrive/DesEval/wiki.en.bin'"
      ],
      "metadata": {
        "id": "arf9whigVvnn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OTKmO3dpNPnn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training and test set\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "test_df = pd.read_csv(TEST_CSV)\n",
        "from nltk.corpus import stopwords\n",
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "def text_to_word_list(text):\n",
        "    ''' Pre process and convert texts to a list of words '''\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    text = text.split()\n",
        "    return text\n",
        "\n",
        "# Prepare embedding\n"
      ],
      "metadata": {
        "id": "SVWG32AwV2qB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = dict()\n",
        "inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
        "fastText = fasttext.load_model(EMBEDDING_FILE)\n",
        "\n",
        "questions_cols = ['question1', 'question2']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bttS1BC_OBda",
        "outputId": "af75c233-f8db-40fe-f89d-0a6cbe0dcf3c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the questions only of both training and test datasets\n",
        "for dataset in [train_df, test_df]:\n",
        "    # print(dataset)\n",
        "    for index, row in dataset.iterrows():\n",
        "\n",
        "        # Iterate through the text of both questions of the row\n",
        "        for question in questions_cols:\n",
        "            # print(question)\n",
        "            q2n = []  # q2n -> question numbers representation\n",
        "            for word in text_to_word_list(row[question]):\n",
        "                # print(word)\n",
        "                # Check for unwanted words\n",
        "                if word in stops and word not in fastText.words :\n",
        "                    continue\n",
        "\n",
        "                if word not in vocabulary:\n",
        "                    vocabulary[word] = len(inverse_vocabulary)\n",
        "                    q2n.append(len(inverse_vocabulary))\n",
        "                    inverse_vocabulary.append(word)\n",
        "                else:\n",
        "                    q2n.append(vocabulary[word])\n",
        "\n",
        "            # Replace questions as word to question as number representation\n",
        "            dataset.at[index, question] = q2n\n",
        "            \n"
      ],
      "metadata": {
        "id": "L9rW7lQzV8Di"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 300\n",
        "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
        "embeddings[0] = 0  # So that the padding will be ignored\n",
        "\n",
        "# Build the embedding matrix\n",
        "for word, index in vocabulary.items():\n",
        "    if word in fastText.words:\n",
        "        embeddings[index] = fastText.get_word_vector(word)\n",
        "\n",
        "del fastText"
      ],
      "metadata": {
        "id": "Z-95xajtP230"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = max(train_df.question1.map(lambda x: len(x)).max(),\n",
        "                     train_df.question2.map(lambda x: len(x)).max(),\n",
        "                     test_df.question1.map(lambda x: len(x)).max(),\n",
        "                     test_df.question2.map(lambda x: len(x)).max())\n",
        "\n",
        "# Split to train validation\n",
        "\n",
        "validation_size = 40000\n",
        "training_size = len(train_df) - validation_size\n",
        "\n",
        "X = train_df[questions_cols]\n",
        "Y = train_df['is_duplicate']\n",
        "\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)\n",
        "\n",
        "# Split to dicts\n",
        "X_train = {'left': X_train.question1, 'right': X_train.question2}\n",
        "X_validation = {'left': X_validation.question1, 'right': X_validation.question2}\n",
        "X_test = {'left': test_df.question1, 'right': test_df.question2}\n",
        "\n",
        "# Convert labels to their numpy representations\n",
        "Y_train = Y_train.values\n",
        "Y_validation = Y_validation.values\n",
        "\n",
        "# Zero padding\n",
        "for dataset, side in itertools.product([X_train, X_validation], ['left', 'right']):\n",
        "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
        "\n",
        "# Make sure everything is ok\n",
        "assert X_train['left'].shape == X_train['right'].shape\n",
        "assert len(X_train['left']) == len(Y_train)"
      ],
      "metadata": {
        "id": "FWn86J2Mc_N-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model variables\n",
        "from tensorflow.keras.optimizers import Adadelta\n",
        "n_hidden = 50\n",
        "gradient_clipping_norm = 1.25\n",
        "batch_size = 64\n",
        "n_epoch = 25\n",
        "\n",
        "def exponent_neg_manhattan_distance(left, right):\n",
        "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
        "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
        "\n",
        "# The visible layer\n",
        "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "\n",
        "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
        "\n",
        "# Embedded version of the inputs\n",
        "encoded_left = embedding_layer(left_input)\n",
        "encoded_right = embedding_layer(right_input)\n",
        "\n",
        "# Since this is a siamese network, both sides share the same LSTM\n",
        "shared_lstm = LSTM(n_hidden)\n",
        "\n",
        "left_output = shared_lstm(encoded_left)\n",
        "right_output = shared_lstm(encoded_right)\n",
        "\n",
        "# Calculates the distance as defined by the MaLSTM model\n",
        "malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
        "\n",
        "# Pack it all up into a model\n",
        "malstm = Model([left_input, right_input], [malstm_distance])\n",
        "\n",
        "# Adadelta optimizer, with gradient clipping by norm\n",
        "optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
        "\n",
        "malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Start training\n",
        "training_start_time = time()\n",
        "\n",
        "malstm_trained = malstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, epochs=n_epoch,\n",
        "                            validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n",
        "\n",
        "print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))"
      ],
      "metadata": {
        "id": "eJ15uUmydBll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2c5734f-a37b-4a40-d683-058ac6af6306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2681/2681 [==============================] - 833s 309ms/step - loss: 0.0023 - accuracy: 0.9980 - val_loss: 0.0023 - val_accuracy: 0.9979\n",
            "Epoch 2/25\n",
            "2681/2681 [==============================] - 822s 306ms/step - loss: 0.0023 - accuracy: 0.9980 - val_loss: 0.0023 - val_accuracy: 0.9979\n",
            "Epoch 3/25\n",
            "2681/2681 [==============================] - 815s 304ms/step - loss: 0.0023 - accuracy: 0.9980 - val_loss: 0.0023 - val_accuracy: 0.9979\n",
            "Epoch 4/25\n",
            "1120/2681 [===========>..................] - ETA: 7:24 - loss: 0.0022 - accuracy: 0.9980"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "malstm_trained.history['accuracy']\n",
        "malstm_trained.history['val_accuracy']"
      ],
      "metadata": {
        "id": "mPBIpHOydERS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot accuracy\n",
        "plt.plot(malstm_trained.history['accuracy'])\n",
        "plt.plot(malstm_trained.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot loss\n",
        "plt.plot(malstm_trained.history['loss'])\n",
        "plt.plot(malstm_trained.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CH7kK2ChdGPh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}