{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert+cosine.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["# File paths\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N61SdFogjs2-","outputId":"3a748170-a9d3-453f-dcd9-5a7d7d821115","executionInfo":{"status":"ok","timestamp":1656521129970,"user_tz":-330,"elapsed":21409,"user":{"displayName":"mrigank khandelwal","userId":"06877974458104855851"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ViGa0fJHgtGr","outputId":"90adf057-3e4a-45f7-95fe-436157fa3fa5","executionInfo":{"status":"ok","timestamp":1656521133420,"user_tz":-330,"elapsed":3481,"user":{"displayName":"mrigank khandelwal","userId":"06877974458104855851"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["***** BERT pretrained directory: gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12 *****\n","gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/bert_config.json\n","gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001\n","gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/bert_model.ckpt.index\n","gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/bert_model.ckpt.meta\n","gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/checkpoint\n","gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/vocab.txt\n","TPU address is grpc://10.32.154.194:8470\n"]}],"source":["import os\n","import sys\n","import json\n","import datetime\n","import pprint\n","\n","\n","\n","\n","# If you want to use TPU, first switch to tpu runtime in colab\n","USE_TPU = True #@param{type:\"boolean\"}\n","\n","# We will use base uncased bert model, you can give try with large models\n","# For large model TPU is necessary\n","BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n","\n","# BERT checkpoint bucket\n","BERT_PRETRAINED_DIR = 'gs://cloud-tpu-checkpoints/bert/' + BERT_MODEL\n","print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n","!gsutil ls $BERT_PRETRAINED_DIR\n","\n","# Bucket for saving checkpoints and outputs\n","\n","\n","if USE_TPU:\n","  # getting info on TPU runtime\n","  assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; Change notebook runtype to TPU'\n","  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","  print('TPU address is', TPU_ADDRESS)\n"]},{"cell_type":"code","source":["# Clone BERT repo and add bert in system path\n","!test -d bert || git clone -q https://github.com/google-research/bert.git\n","if not 'bert' in sys.path:\n","  sys.path += ['bert']"],"metadata":{"id":"n9MKuwP_kjEj","executionInfo":{"status":"ok","timestamp":1656521777128,"user_tz":-330,"elapsed":1356,"user":{"displayName":"mrigank khandelwal","userId":"06877974458104855851"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Model Hyper Parameters\n","TRAIN_BATCH_SIZE = 32 # For GPU, reduce to 16\n","EVAL_BATCH_SIZE = 8\n","PREDICT_BATCH_SIZE = 8\n","LEARNING_RATE = 2e-5\n","NUM_TRAIN_EPOCHS = 2.0\n","WARMUP_PROPORTION = 0.1\n","MAX_SEQ_LENGTH = 200 \n","\n","# Model configs\n","SAVE_CHECKPOINTS_STEPS = 1000\n","ITERATIONS_PER_LOOP = 1000\n","NUM_TPU_CORES = 8\n","VOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n","CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n","INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n","DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n"],"metadata":{"id":"V4DPD6lBkyUQ","executionInfo":{"status":"ok","timestamp":1656521875626,"user_tz":-330,"elapsed":410,"user":{"displayName":"mrigank khandelwal","userId":"06877974458104855851"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#!pip uninstall tensorflow==2.2.0\n","\n"],"metadata":{"id":"fv-rG-wnqZCP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tensorflow==1.15.0\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6oTYmlyUqtun","outputId":"b67f18b5-f12f-4702-e19b-591a16f4596b","executionInfo":{"status":"ok","timestamp":1656522005684,"user_tz":-330,"elapsed":90124,"user":{"displayName":"mrigank khandelwal","userId":"06877974458104855851"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow==1.15.0\n","  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n","\u001b[K     |████████████████████████████████| 412.3 MB 14 kB/s \n","\u001b[?25hCollecting gast==0.2.2\n","  Downloading gast-0.2.2.tar.gz (10 kB)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.14.1)\n","Collecting tensorflow-estimator==1.15.1\n","  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n","\u001b[K     |████████████████████████████████| 503 kB 22.4 MB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.46.3)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.17.3)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.37.1)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.21.6)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 30.2 MB/s \n","\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n","Collecting keras-applications>=1.0.8\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 4.4 MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (3.1.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.7)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (57.4.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.11.4)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.8.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.0) (1.5.2)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=209a25ab3cc49552414389145e0c6b4769beb3fe7057c7d1b896562964622aa7\n","  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n","Successfully built gast\n","Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.8.0\n","    Uninstalling tensorboard-2.8.0:\n","      Successfully uninstalled tensorboard-2.8.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.3\n","    Uninstalling gast-0.5.3:\n","      Successfully uninstalled gast-0.5.3\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n","    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n","      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n","kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"]}]},{"cell_type":"code","source":["!pip install bert-tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OHAEARXgqzIY","outputId":"36c01d8d-30ac-4f60-cc72-8d1155239768","executionInfo":{"status":"ok","timestamp":1656522009579,"user_tz":-330,"elapsed":3907,"user":{"displayName":"mrigank khandelwal","userId":"06877974458104855851"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting bert-tensorflow\n","  Downloading bert_tensorflow-1.0.4-py2.py3-none-any.whl (64 kB)\n","\u001b[K     |████████████████████████████████| 64 kB 1.7 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from bert-tensorflow) (1.15.0)\n","Installing collected packages: bert-tensorflow\n","Successfully installed bert-tensorflow-1.0.4\n"]}]},{"cell_type":"code","source":["import tensorflow as tf"],"metadata":{"id":"X3hWQn2IU7MX","executionInfo":{"status":"ok","timestamp":1656522012642,"user_tz":-330,"elapsed":3079,"user":{"displayName":"mrigank khandelwal","userId":"06877974458104855851"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import modeling\n","import optimization\n","import tokenization\n","import run_classifier"],"metadata":{"id":"hWi2nB-bqUw9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656522012642,"user_tz":-330,"elapsed":15,"user":{"displayName":"mrigank khandelwal","userId":"06877974458104855851"}},"outputId":"4b6c404b-44e8-4c29-f97a-438c0d4c8a73"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"]}]},{"cell_type":"code","source":["class QQPProcessor(run_classifier.DataProcessor):\n","  \"\"\"Processor for the Quora Question pair data set.\"\"\"\n","\n","  def get_train_examples(self, data_dir):\n","    \"\"\"Reading train.tsv and converting to list of InputExample\"\"\"\n","    print(\"here in train\")\n","    return self._create_examples(\n","        self._read_tsv(os.path.join(data_dir,\"train.tsv\")), 'train')\n","\n","  def get_dev_examples(self, data_dir):\n","    \"\"\"Reading dev.tsv and converting to list of InputExample\"\"\"\n","    return self._create_examples(\n","        self._read_tsv(os.path.join(data_dir,\"train.tsv\")), 'dev')\n","  \n","  def get_test_examples(self, data_dir):\n","    \"\"\"Reading train.tsv and converting to list of InputExample\"\"\"\n","    return self._create_examples(\n","        self._read_tsv(os.path.join(data_dir,\"train.tsv\")), 'test')\n","  \n","  def get_predict_examples(self, sentence_pairs):\n","    \"\"\"Given question pairs, conevrting to list of InputExample\"\"\"\n","    examples = []\n","    for (i, qpair) in enumerate(sentence_pairs):\n","      guid = \"predict-%d\" % (i)\n","      print(guid)\n","      # converting questions to utf-8 and creating InputExamples\n","      text_a = tokenization.convert_to_unicode(qpair[0])\n","      text_b = tokenization.convert_to_unicode(qpair[1])\n","      # We will add label  as 0, because None is not supported in converting to features\n","      examples.append(\n","          run_classifier.InputExample(guid=guid, text_a=text_a, text_b=text_b, label=0))\n","    return examples\n","  \n","  def _create_examples(self, lines, set_type):\n","    \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n","    examples = []\n","    #print(\"hello\")\n","    for (i, line) in enumerate(lines):\n","      guid = \"%s-%d\" % (set_type, i)\n","     # print(guid)\n","      if set_type=='test':\n","        # removing header and invalid data\n","        if i == 0 or len(line)!=3:\n","          #print(guid, line)\n","          continue\n","        text_a = tokenization.convert_to_unicode(line[1])\n","        text_b = tokenization.convert_to_unicode(line[2])\n","        label = 0 # We will use zero for test as convert_example_to_features doesn't support None\n","      else:\n","        # removing header and invalid data\n","        if i == 0 :\n","         # print(i,len(line))\n","          continue\n","        text_a = tokenization.convert_to_unicode(line[1])\n","        text_b = tokenization.convert_to_unicode(line[2])\n","        label = int(line[3])\n","        #print(text_a,text_b,label)\n","      examples.append(\n","          run_classifier.InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n","    return examples\n","\n","  def get_labels(self):\n","    \"return class labels\"\n","    return [0,1]"],"metadata":{"id":"PjVkIGmsqMm6","executionInfo":{"status":"ok","timestamp":1656522012643,"user_tz":-330,"elapsed":13,"user":{"displayName":"mrigank khandelwal","userId":"06877974458104855851"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Instantiate an instance of QQPProcessor and tokenizer\n","processor = QQPProcessor()\n","label_list = processor.get_labels()\n","tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\n","#train_examples = processor.get_train_examples(TASK_DATA_DIR)"],"metadata":{"id":"kLsohTVCTbwA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656522012643,"user_tz":-330,"elapsed":12,"user":{"displayName":"mrigank khandelwal","userId":"06877974458104855851"}},"outputId":"17b3d00d-07e5-4881-d2ea-85d468e29806"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n"]}]},{"cell_type":"code","source":["TASK_DATA_DIR = '/content/drive/MyDrive/DesEval'\n","OUTPUT_DIR = '/content/drive/MyDrive/DesEval/op'"],"metadata":{"id":"ePH5ilaKVEt4","executionInfo":{"status":"ok","timestamp":1656522041789,"user_tz":-330,"elapsed":488,"user":{"displayName":"mrigank khandelwal","userId":"06877974458104855851"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["#OUTPUT_DIR = 'out_dir'\n","# os.mkdir(OUTPUT_DIR)\n","print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cci1RqWiXHhp","outputId":"32d76538-2c49-446f-e687-1e856b4067b4","executionInfo":{"status":"ok","timestamp":1656522044786,"user_tz":-330,"elapsed":414,"user":{"displayName":"mrigank khandelwal","userId":"06877974458104855851"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["***** Model output directory: /content/drive/MyDrive/DesEval/op *****\n"]}]},{"cell_type":"code","source":["train_examples = processor.get_train_examples(TASK_DATA_DIR)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r6II5wCHbFBd","outputId":"f839bad6-f9ad-41d6-ba1c-ff348e2b8e8c","executionInfo":{"status":"ok","timestamp":1656522052475,"user_tz":-330,"elapsed":5129,"user":{"displayName":"mrigank khandelwal","userId":"06877974458104855851"}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["here in train\n","WARNING:tensorflow:From bert/run_classifier.py:199: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n","\n"]}]},{"cell_type":"code","source":["# Converting training examples to features\n","print(\"################  Processing Training Data #####################\")\n","TRAIN_TF_RECORD = os.path.join(OUTPUT_DIR, \"train.tf_record\")\n","train_examples = processor.get_train_examples(TASK_DATA_DIR)\n","num_train_examples = len(train_examples)\n","num_train_steps = int( num_train_examples / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","run_classifier.file_based_convert_examples_to_features(train_examples, label_list, MAX_SEQ_LENGTH, tokenizer, TRAIN_TF_RECORD)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"id":"o2GKghOHTpIi","outputId":"74916576-ff76-400b-b5c7-b74ec8734976","executionInfo":{"status":"error","timestamp":1656522059021,"user_tz":-330,"elapsed":6552,"user":{"displayName":"mrigank khandelwal","userId":"06877974458104855851"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["################  Processing Training Data #####################\n","here in train\n","WARNING:tensorflow:From bert/run_classifier.py:483: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n","\n"]},{"output_type":"error","ename":"NotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-408fffe74326>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnum_train_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnum_train_examples\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mTRAIN_BATCH_SIZE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mNUM_TRAIN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnum_warmup_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train_steps\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mWARMUP_PROPORTION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrun_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_based_convert_examples_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SEQ_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_TF_RECORD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/bert/run_classifier.py\u001b[0m in \u001b[0;36mfile_based_convert_examples_to_features\u001b[0;34m(examples, label_list, max_seq_length, tokenizer, output_file)\u001b[0m\n\u001b[1;32m    481\u001b[0m   \u001b[0;34m\"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m   \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mex_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/lib/io/tf_record.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, options)\u001b[0m\n\u001b[1;32m    216\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m       self._writer = pywrap_tensorflow.PyRecordWriter_New(\n\u001b[0;32m--> 218\u001b[0;31m           compat.as_bytes(path), options._as_record_writer_options(), status)\n\u001b[0m\u001b[1;32m    219\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    557\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: /content/drive/MyDrive/DesEval/op/train.tf_record; No such file or directory"]}]},{"cell_type":"code","source":["print(num_train_examples)"],"metadata":{"id":"CkCkevpnbbLg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n","                 labels, num_labels, use_one_hot_embeddings):\n","  \"\"\"Creates a classification model.\"\"\"\n","  # Bert Model instant \n","  model = modeling.BertModel(\n","      config=bert_config,\n","      is_training=is_training,\n","      input_ids=input_ids,\n","      input_mask=input_mask,\n","      token_type_ids=segment_ids,\n","      use_one_hot_embeddings=use_one_hot_embeddings)\n","\n","  # Getting output for last layer of BERT\n","  output_layer = model.get_pooled_output()\n","  \n","  # Number of outputs for last layer\n","  hidden_size = output_layer.shape[-1].value\n","  \n","  # We will use one layer on top of BERT pretrained for creating classification model\n","  output_weights = tf.get_variable(\n","      \"output_weights\", [num_labels, hidden_size],\n","      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","\n","  output_bias = tf.get_variable(\n","      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n","\n","  with tf.variable_scope(\"loss\"):\n","    if is_training:\n","      # 0.1 dropout\n","      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n","    \n","    # Calcaulte prediction probabilites and loss\n","    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n","    logits = tf.nn.bias_add(logits, output_bias)\n","    probabilities = tf.nn.softmax(logits, axis=-1)\n","    log_probs = tf.nn.log_softmax(logits, axis=-1)\n","\n","    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n","\n","    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n","    loss = tf.reduce_mean(per_example_loss)\n","\n","    return (loss, per_example_loss, logits, probabilities)"],"metadata":{"id":"0OMMpCz5Tt1V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n","                     num_train_steps, num_warmup_steps, use_tpu,\n","                     use_one_hot_embeddings):\n","  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","\n","  def model_fn(features, labels, mode, params):  \n","    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","    # reading features input\n","    input_ids = features[\"input_ids\"]\n","    input_mask = features[\"input_mask\"]\n","    segment_ids = features[\"segment_ids\"]\n","    label_ids = features[\"label_ids\"]\n","    is_real_example = None\n","    if \"is_real_example\" in features:\n","      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n","    else:\n","      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n","    \n","    # checking if training mode\n","    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n","    \n","    # create simple classification model\n","    (total_loss, per_example_loss, logits, probabilities) = create_model(\n","        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n","        num_labels, use_one_hot_embeddings)\n","    \n","    # getting variables for intialization and using pretrained init checkpoint\n","    tvars = tf.trainable_variables()\n","    initialized_variable_names = {}\n","    scaffold_fn = None\n","    if init_checkpoint:\n","      (assignment_map, initialized_variable_names\n","      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n","      if use_tpu:\n","\n","        def tpu_scaffold():\n","          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n","          return tf.train.Scaffold()\n","\n","        scaffold_fn = tpu_scaffold\n","      else:\n","        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n","\n","    output_spec = None\n","    if mode == tf.estimator.ModeKeys.TRAIN:\n","      # defining optimizar function\n","      train_op = optimization.create_optimizer(\n","          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n","      \n","      # Training estimator spec\n","      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n","          mode=mode,\n","          loss=total_loss,\n","          train_op=train_op,\n","          scaffold_fn=scaffold_fn)\n","    elif mode == tf.estimator.ModeKeys.EVAL:\n","      # accuracy, loss, auc, F1, precision and recall metrics for evaluation\n","      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n","        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n","        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n","        accuracy = tf.metrics.accuracy(\n","            labels=label_ids, predictions=predictions, weights=is_real_example)\n","        f1_score = tf.contrib.metrics.f1_score(\n","            label_ids,\n","            predictions)\n","        auc = tf.metrics.auc(\n","            label_ids,\n","            predictions)\n","        recall = tf.metrics.recall(\n","            label_ids,\n","            predictions)\n","        precision = tf.metrics.precision(\n","            label_ids,\n","            predictions) \n","        return {\n","            \"eval_accuracy\": accuracy,\n","            \"eval_loss\": loss,\n","            \"f1_score\": f1_score,\n","            \"auc\": auc,\n","            \"precision\": precision,\n","            \"recall\": recall\n","        }\n","\n","      eval_metrics = (metric_fn,\n","                      [per_example_loss, label_ids, logits, is_real_example])\n","      # estimator spec for evalaution\n","      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n","          mode=mode,\n","          loss=total_loss,\n","          eval_metrics=eval_metrics,\n","          scaffold_fn=scaffold_fn)\n","    else:\n","      # estimator spec for predictions\n","      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n","          mode=mode,\n","          predictions={\"probabilities\": probabilities},\n","          scaffold_fn=scaffold_fn)\n","    return output_spec\n","\n","  return model_fn"],"metadata":{"id":"Ik2P8iQ_TvfJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define TPU configs\n","if USE_TPU:\n","  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n","else:\n","  tpu_cluster_resolver = None\n","run_config = tf.contrib.tpu.RunConfig(\n","    cluster=tpu_cluster_resolver,\n","    model_dir=OUTPUT_DIR,\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n","    tpu_config=tf.contrib.tpu.TPUConfig(\n","        iterations_per_loop=ITERATIONS_PER_LOOP,\n","        num_shards=NUM_TPU_CORES,\n","        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))"],"metadata":{"id":"hpIYgQWITxT6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create model function for estimator using model function builder\n","model_fn = model_fn_builder(\n","    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n","    num_labels=len(label_list),\n","    init_checkpoint=INIT_CHECKPOINT,\n","    learning_rate=LEARNING_RATE,\n","    num_train_steps=num_train_steps,\n","    num_warmup_steps=num_warmup_steps,\n","    use_tpu=USE_TPU,\n","    use_one_hot_embeddings=True)"],"metadata":{"id":"tAvuarxsTy6R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Defining TPU Estimator\n","estimator = tf.contrib.tpu.TPUEstimator(\n","    use_tpu=USE_TPU,\n","    model_fn=model_fn,\n","    config=run_config,\n","    train_batch_size=TRAIN_BATCH_SIZE,\n","    eval_batch_size=EVAL_BATCH_SIZE,\n","    predict_batch_size=PREDICT_BATCH_SIZE)"],"metadata":{"id":"-GgdI9gLT0ba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model.\n","print('QQP on BERT base model normally takes about 1 hour on TPU and 15-20 hours on GPU. Please wait...')\n","print('***** Started training at {} *****'.format(datetime.datetime.now()))\n","print('  Num examples = {}'.format(num_train_examples))\n","print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n","tf.logging.info(\"  Num steps = %d\", num_train_steps)\n","# we are using `file_based_input_fn_builder` for creating input function from TF_RECORD file\n","train_input_fn = run_classifier.file_based_input_fn_builder(TRAIN_TF_RECORD,\n","                                                            seq_length=MAX_SEQ_LENGTH,\n","                                                            is_training=True,\n","                                                            drop_remainder=True)\n"],"metadata":{"id":"cC4SaI0cT2hk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","print('***** Finished training at {} *****'.format(datetime.datetime.now()))"],"metadata":{"id":"PEuVPpdAhO2L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('***** Started Train Set evaluation at {} *****'.format(datetime.datetime.now()))\n","print('  Num examples = {}'.format(num_train_examples))\n","print('  Batch size = {}'.format(EVAL_BATCH_SIZE))\n","# eval input function for train set\n","train_eval_input_fn = run_classifier.file_based_input_fn_builder(TRAIN_TF_RECORD,\n","                                                           seq_length=MAX_SEQ_LENGTH,\n","                                                           is_training=False,\n","                                                           drop_remainder=True)\n","# evalute on train set\n","result = estimator.evaluate(input_fn=train_eval_input_fn, \n","                            steps=int(num_train_examples/EVAL_BATCH_SIZE))\n","print('***** Finished evaluation at {} *****'.format(datetime.datetime.now()))\n","print(\"***** Eval results *****\")\n","for key in sorted(result.keys()):\n","  print('  {} = {}'.format(key, str(result[key])))"],"metadata":{"id":"lTEZCqY5gh76"},"execution_count":null,"outputs":[]}]}